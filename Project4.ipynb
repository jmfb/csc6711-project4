{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d20c0a-a47e-4e52-95e5-02289c0e21c2",
   "metadata": {},
   "source": [
    "# CSC6711 Project 4 - Collaborative Filtering with kNN\n",
    "* **Author:** Jacob Buysse\n",
    "\n",
    "This notebook is an analysis of the predictions based on user clustering using kNN.\n",
    "The files are located in the datasets subdirectory:\n",
    "\n",
    "* MovieLens - `movielens_25m.feather` (Movies)\n",
    "* Netflix Prize - `netflix_prize.feather` (Movies and TV Shows)\n",
    "* Yahoo! Music R2 - `yahoo_r2_songs.subsampled.feather` (Songs)\n",
    "* BoardGameGeek - `boardgamegeek.feather` (Board Games)\n",
    "\n",
    "We will be using the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b34c046a-057c-445d-968f-9c704927088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03909103-b288-46cb-a2ee-db0b54ac1ca4",
   "metadata": {},
   "source": [
    "Let us configure matplotlib for readable labels, high resolution, and automatic layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc1e42e-1c37-45c9-9105-e4e88a97e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('axes', labelsize=16)\n",
    "matplotlib.rc('figure', dpi=150, autolayout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e39bc-aba9-4bf1-8847-41e05f0c2e9e",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let us load the 4 datasets.  We will proceed to clean, filter, preprocess, and split the datasets before continuing on to the kNN portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ed13165-04a0-426e-b06e-a74e5ce0df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens...\n",
      "Shape (24890583, 4)\n",
      "Loading Netflix...\n",
      "Shape (51031355, 4)\n",
      "Loading Yahoo! Music...\n",
      "Shape (6937275, 4)\n",
      "Loading BoardGameGeek...\n",
      "Encoding user_id: string -> int64\n",
      "Shape (18942215, 4)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    { 'Title': 'MovieLens', 'File': 'movielens_25m' },\n",
    "    { 'Title': 'Netflix', 'File': 'netflix_prize' },\n",
    "    { 'Title': 'Yahoo! Music', 'File': 'yahoo_r2_songs.subsampled' },\n",
    "    { 'Title': 'BoardGameGeek', 'File': 'boardgamegeek' }\n",
    "]\n",
    "for dataset in datasets:\n",
    "    # Load the file\n",
    "    print(f\"Loading {dataset['Title']}...\")\n",
    "    df = pd.read_feather(f\"./datasets/{dataset['File']}.feather\")\n",
    "\n",
    "    # Add a rating_bin (floor of the rating) for graphing bins later\n",
    "    df['rating_bin'] = np.floor(df.rating)\n",
    "\n",
    "    # Use the label encoder to convert user_id into a numeric when it is a string (object)\n",
    "    # NOTE: This is needed for the BoardGameGeek dataset\n",
    "    if (df.user_id.dtype == object):\n",
    "        print('Encoding user_id: string -> int64')\n",
    "        user_id_encoder = LabelEncoder()\n",
    "        user_id_encoder.fit(df.user_id)\n",
    "        dataset['user_id_encoder'] = user_id_encoder\n",
    "        df['user_id'] = user_id_encoder.transform(df.user_id)\n",
    "\n",
    "    # Store the df in the dataset dictionary\n",
    "    dataset['df'] = df\n",
    "    print(f\"Shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e76b2-ff5f-4d9f-8c99-9dd2dca5ea72",
   "metadata": {},
   "source": [
    "Because of how we are doing our training/testing split and then seen/unseen split for testing, we need to exclude all users that only have a single rating.  This is because we cannot split those users in the testing dataset into both seen (for matching neighbors) and unseen (for prediction analysis).  Let us filter those users out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10461f9-da4e-4b0e-93cd-24a9f2813905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with only one rating for MovieLens\n",
      "New Shape: (24890583, 5)\n",
      "Filtering out users with only one rating for Netflix\n",
      "New Shape: (51027153, 5)\n",
      "Filtering out users with only one rating for Yahoo! Music\n",
      "New Shape: (6532945, 5)\n",
      "Filtering out users with only one rating for BoardGameGeek\n",
      "New Shape: (18862919, 5)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Filtering out users with only one rating for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    counts_df = df.groupby('user_id')[['rating']].count()\n",
    "    merged_df = df.merge(counts_df, on='user_id', suffixes=['', '_count'])\n",
    "    filtered_df = merged_df[merged_df.rating_count > 1]\n",
    "    dataset['df'] = filtered_df.copy()\n",
    "    print(f\"New Shape: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd883aeb-28a9-4bdd-a7b3-7a6693f25345",
   "metadata": {},
   "source": [
    "Let us encode the `item_id` column into a contiguous 0...n-1 range `item_idx` using a LabelEncoder.\n",
    "This will be used for the columns of the sparse matrices.\n",
    "Note that this encoding will be shared between the training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e6aa8b7-fc71-46fd-887e-17d715318453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id for MovieLens\n",
      "Distinct Item Count: 24,330\n",
      "Encoding item_id for Netflix\n",
      "Distinct Item Count: 9,210\n",
      "Encoding item_id for Yahoo! Music\n",
      "Distinct Item Count: 1,368\n",
      "Encoding item_id for BoardGameGeek\n",
      "Distinct Item Count: 21,925\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding item_id for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    item_id_encoder = LabelEncoder()\n",
    "    item_id_encoder.fit(df.item_id)\n",
    "    dataset['item_id_encoder'] = item_id_encoder\n",
    "    n_items = item_id_encoder.classes_.size\n",
    "    dataset['n_items'] = n_items\n",
    "    df['item_idx'] = item_id_encoder.transform(df.item_id)\n",
    "    print(f\"Distinct Item Count: {n_items:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82f9ee-8cfe-41ee-8442-86968fa4818c",
   "metadata": {},
   "source": [
    "Let us do a 75/25 split for the training/testing datasets, split across the user ids as groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8988d88-69d5-4618-92d2-f5222e985986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training/testing datasets for MovieLens\n",
      "Train (18706943, 6) (75% total, 100% items, 75% users) Test (6183640, 6) (25% total, 99% items, 25% users)\n",
      "Splitting training/testing datasets for Netflix\n",
      "Train (38292233, 6) (75% total, 100% items, 75% users) Test (12734920, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for Yahoo! Music\n",
      "Train (4901087, 6) (75% total, 100% items, 75% users) Test (1631858, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for BoardGameGeek\n",
      "Train (14119520, 6) (75% total, 100% items, 75% users) Test (4743399, 6) (25% total, 100% items, 25% users)\n"
     ]
    }
   ],
   "source": [
    "def TrainTestSplit(df):\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=777)\n",
    "    train_index, test_index = next(gss.split(X=df, y=df.rating, groups=df.user_id))\n",
    "    train_df = df.iloc[train_index].copy()\n",
    "    test_df = df.iloc[test_index].copy()\n",
    "    total_count = train_df.shape[0] + test_df.shape[0];\n",
    "    item_count = df.item_id.nunique()\n",
    "    user_count = df.user_id.nunique()\n",
    "    train_pct_total = train_df.shape[0] / total_count\n",
    "    test_pct_total = test_df.shape[0] / total_count\n",
    "    train_pct_item = train_df.item_id.nunique() / item_count\n",
    "    test_pct_item = test_df.item_id.nunique() / item_count\n",
    "    train_pct_user = train_df.user_id.nunique() / user_count\n",
    "    test_pct_user = test_df.user_id.nunique() / user_count\n",
    "    print(f\"Train {train_df.shape} ({train_pct_total:.0%} total, {train_pct_item:.0%} items, {train_pct_user:.0%} users) \" +\n",
    "          f\"Test {test_df.shape} ({test_pct_total:.0%} total, {test_pct_item:.0%} items, {test_pct_user:.0%} users)\")\n",
    "    return train_df, test_df\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Splitting training/testing datasets for {dataset['Title']}\")\n",
    "    train_df, test_df = TrainTestSplit(dataset['df'])\n",
    "    dataset['train_df'] = train_df\n",
    "    dataset['test_df'] = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66dd2a-27c9-43f9-8781-5782bd421198",
   "metadata": {},
   "source": [
    "Now, for each training/testing data frame, encode the user ids to contiguous sets using LabelEncoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c0045c5-ccad-422b-aaac-30701dafe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding user_id for MovieLens\n",
      "Distinct Training Users: 121,905\n",
      "Distinct Testing Users: 40,636\n",
      "Encoding user_id for Netflix\n",
      "Distinct Training Users: 355,362\n",
      "Distinct Testing Users: 118,454\n",
      "Encoding user_id for Yahoo! Music\n",
      "Distinct Training Users: 638,199\n",
      "Distinct Testing Users: 212,733\n",
      "Encoding user_id for BoardGameGeek\n",
      "Distinct Training Users: 249,059\n",
      "Distinct Testing Users: 83,020\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding user_id for {dataset['Title']}\")\n",
    "\n",
    "    train_df = dataset['train_df']\n",
    "    train_user_id_encoder = LabelEncoder()\n",
    "    train_user_id_encoder.fit(train_df.user_id)\n",
    "    dataset['train_user_id_encoder'] = train_user_id_encoder\n",
    "    n_train_users = train_user_id_encoder.classes_.size\n",
    "    dataset['n_train_users'] = n_train_users\n",
    "    train_df['user_idx'] = train_user_id_encoder.transform(train_df.user_id)\n",
    "    print(f\"Distinct Training Users: {n_train_users:,}\")\n",
    "\n",
    "    test_df = dataset['test_df']\n",
    "    test_user_id_encoder = LabelEncoder()\n",
    "    test_user_id_encoder.fit(test_df.user_id)\n",
    "    dataset['test_user_id_encoder'] = test_user_id_encoder\n",
    "    n_test_users = test_user_id_encoder.classes_.size\n",
    "    dataset['n_test_users'] = n_test_users\n",
    "    test_df['user_idx'] = test_user_id_encoder.transform(test_df.user_id)\n",
    "    print(f\"Distinct Testing Users: {n_test_users:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989f3c6-534c-49a7-bcae-6987dd391dc6",
   "metadata": {},
   "source": [
    "Now create a sparse matrix for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a529fa22-d031-42be-b091-0b5e5f959e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse training matrix for MovieLens\n",
      "Creating sparse training matrix for Netflix\n",
      "Creating sparse training matrix for Yahoo! Music\n",
      "Creating sparse training matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse training matrix for {dataset['Title']}\")\n",
    "    dataset['train_X'] = csr_matrix(\n",
    "        (dataset['train_df'].rating, (dataset['train_df'].user_idx, dataset['train_df'].item_idx)),\n",
    "        shape=(dataset['n_train_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31334078-351a-41a7-8bc0-239a57a4e340",
   "metadata": {},
   "source": [
    "Now we need to split the test dataset into a seen/unseen split (75/25), stratified by user id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d851b0ea-772b-4478-8e6a-0597740107d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating seen/unseen testing split MovieLens\n",
      "Seen (4637730, 7), Unseen (1545910, 7)\n",
      "Creating seen/unseen testing split Netflix\n",
      "Seen (9551190, 7), Unseen (3183730, 7)\n",
      "Creating seen/unseen testing split Yahoo! Music\n",
      "Seen (1223893, 7), Unseen (407965, 7)\n",
      "Creating seen/unseen testing split BoardGameGeek\n",
      "Seen (3557549, 7), Unseen (1185850, 7)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating seen/unseen testing split {dataset['Title']}\")\n",
    "    test_df = dataset['test_df']\n",
    "    seen_df, unseen_df = train_test_split(test_df, train_size=0.75, random_state=777, stratify=test_df.user_idx)\n",
    "    dataset['seen_df'] = seen_df\n",
    "    dataset['unsee_df'] = unseen_df\n",
    "    print(f\"Seen {seen_df.shape}, Unseen {unseen_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce5558-d2de-4c8b-8487-558a013f5429",
   "metadata": {},
   "source": [
    "Now we can create the sparse matrix for the testing seen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d277d07-8cec-4f50-86d5-121829913ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse testing/seen matrix for MovieLens\n",
      "Creating sparse testing/seen matrix for Netflix\n",
      "Creating sparse testing/seen matrix for Yahoo! Music\n",
      "Creating sparse testing/seen matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse testing/seen matrix for {dataset['Title']}\")\n",
    "    dataset['test_X'] = csr_matrix(\n",
    "        (dataset['seen_df'].rating, (dataset['seen_df'].user_idx, dataset['seen_df'].item_idx)),\n",
    "        shape=(dataset['n_test_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c251a-4cbb-43d3-ad21-653fe2880102",
   "metadata": {},
   "source": [
    "## TODO: Left off here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be04422-80b1-44f9-9fad-e7dc443ffe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
