{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d20c0a-a47e-4e52-95e5-02289c0e21c2",
   "metadata": {},
   "source": [
    "# CSC6711 Project 4 - Collaborative Filtering with kNN\n",
    "* **Author:** Jacob Buysse\n",
    "\n",
    "This notebook is an analysis of the predictions based on user clustering using kNN.\n",
    "The files are located in the datasets subdirectory:\n",
    "\n",
    "* MovieLens - `movielens_25m.feather` (Movies)\n",
    "* Netflix Prize - `netflix_prize.feather` (Movies and TV Shows)\n",
    "* Yahoo! Music R2 - `yahoo_r2_songs.subsampled.feather` (Songs)\n",
    "* BoardGameGeek - `boardgamegeek.feather` (Board Games)\n",
    "\n",
    "We will be using the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34c046a-057c-445d-968f-9c704927088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03909103-b288-46cb-a2ee-db0b54ac1ca4",
   "metadata": {},
   "source": [
    "Let us configure matplotlib for readable labels, high resolution, and automatic layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc1e42e-1c37-45c9-9105-e4e88a97e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('axes', labelsize=16)\n",
    "matplotlib.rc('figure', dpi=150, autolayout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e39bc-aba9-4bf1-8847-41e05f0c2e9e",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let us load the 4 datasets.  We will proceed to clean, filter, preprocess, and split the datasets before continuing on to the kNN portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed13165-04a0-426e-b06e-a74e5ce0df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens...\n",
      "Shape (24890583, 4)\n",
      "Loading Netflix...\n",
      "Shape (51031355, 4)\n",
      "Loading Yahoo! Music...\n",
      "Shape (6937275, 4)\n",
      "Loading BoardGameGeek...\n",
      "Encoding user_id: string -> int64\n",
      "Shape (18942215, 4)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    { 'Title': 'MovieLens', 'File': 'movielens_25m' },\n",
    "    { 'Title': 'Netflix', 'File': 'netflix_prize' },\n",
    "    { 'Title': 'Yahoo! Music', 'File': 'yahoo_r2_songs.subsampled' },\n",
    "    { 'Title': 'BoardGameGeek', 'File': 'boardgamegeek' }\n",
    "]\n",
    "for dataset in datasets:\n",
    "    # Load the file\n",
    "    print(f\"Loading {dataset['Title']}...\")\n",
    "    df = pd.read_feather(f\"./datasets/{dataset['File']}.feather\")\n",
    "\n",
    "    # Add a rating_bin (floor of the rating) for graphing bins later\n",
    "    df['rating_bin'] = np.floor(df.rating)\n",
    "\n",
    "    # Use the label encoder to convert user_id into a numeric when it is a string (object)\n",
    "    # NOTE: This is needed for the BoardGameGeek dataset\n",
    "    if (df.user_id.dtype == object):\n",
    "        print('Encoding user_id: string -> int64')\n",
    "        user_id_encoder = LabelEncoder()\n",
    "        user_id_encoder.fit(df.user_id)\n",
    "        dataset['user_id_encoder'] = user_id_encoder\n",
    "        df['user_id'] = user_id_encoder.transform(df.user_id)\n",
    "\n",
    "    # Store the df in the dataset dictionary\n",
    "    dataset['df'] = df\n",
    "    print(f\"Shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e76b2-ff5f-4d9f-8c99-9dd2dca5ea72",
   "metadata": {},
   "source": [
    "Because of how we are doing our training/testing split and then seen/unseen split for testing, we need to exclude all users that only have a single rating.  This is because we cannot split those users in the testing dataset into both seen (for matching neighbors) and unseen (for prediction analysis).  Let us filter those users out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10461f9-da4e-4b0e-93cd-24a9f2813905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with only one rating for MovieLens\n",
      "New Shape: (24890583, 5)\n",
      "Filtering out users with only one rating for Netflix\n",
      "New Shape: (51027153, 5)\n",
      "Filtering out users with only one rating for Yahoo! Music\n",
      "New Shape: (6532945, 5)\n",
      "Filtering out users with only one rating for BoardGameGeek\n",
      "New Shape: (18862919, 5)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Filtering out users with only one rating for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    counts_df = df.groupby('user_id')[['rating']].count()\n",
    "    merged_df = df.merge(counts_df, on='user_id', suffixes=['', '_count'])\n",
    "    filtered_df = merged_df[merged_df.rating_count > 1]\n",
    "    dataset['df'] = filtered_df.copy()\n",
    "    print(f\"New Shape: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd883aeb-28a9-4bdd-a7b3-7a6693f25345",
   "metadata": {},
   "source": [
    "Let us encode the `item_id` column into a contiguous 0...n-1 range `item_idx` using a LabelEncoder.\n",
    "This will be used for the columns of the sparse matrices.\n",
    "Note that this encoding will be shared between the training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6aa8b7-fc71-46fd-887e-17d715318453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id for MovieLens\n",
      "Distinct Item Count: 24,330\n",
      "Encoding item_id for Netflix\n",
      "Distinct Item Count: 9,210\n",
      "Encoding item_id for Yahoo! Music\n",
      "Distinct Item Count: 1,368\n",
      "Encoding item_id for BoardGameGeek\n",
      "Distinct Item Count: 21,925\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding item_id for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    item_id_encoder = LabelEncoder()\n",
    "    item_id_encoder.fit(df.item_id)\n",
    "    dataset['item_id_encoder'] = item_id_encoder\n",
    "    n_items = item_id_encoder.classes_.size\n",
    "    dataset['n_items'] = n_items\n",
    "    df['item_idx'] = item_id_encoder.transform(df.item_id)\n",
    "    print(f\"Distinct Item Count: {n_items:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82f9ee-8cfe-41ee-8442-86968fa4818c",
   "metadata": {},
   "source": [
    "Let us do a 75/25 split for the training/testing datasets, split across the user ids as groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8988d88-69d5-4618-92d2-f5222e985986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training/testing datasets for MovieLens\n",
      "Train (18706943, 6) (75% total, 100% items, 75% users) Test (6183640, 6) (25% total, 99% items, 25% users)\n",
      "Splitting training/testing datasets for Netflix\n",
      "Train (38292233, 6) (75% total, 100% items, 75% users) Test (12734920, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for Yahoo! Music\n",
      "Train (4901087, 6) (75% total, 100% items, 75% users) Test (1631858, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for BoardGameGeek\n",
      "Train (14119520, 6) (75% total, 100% items, 75% users) Test (4743399, 6) (25% total, 100% items, 25% users)\n"
     ]
    }
   ],
   "source": [
    "def TrainTestSplit(df):\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=777)\n",
    "    train_index, test_index = next(gss.split(X=df, y=df.rating, groups=df.user_id))\n",
    "    train_df = df.iloc[train_index].copy()\n",
    "    test_df = df.iloc[test_index].copy()\n",
    "    total_count = train_df.shape[0] + test_df.shape[0];\n",
    "    item_count = df.item_id.nunique()\n",
    "    user_count = df.user_id.nunique()\n",
    "    train_pct_total = train_df.shape[0] / total_count\n",
    "    test_pct_total = test_df.shape[0] / total_count\n",
    "    train_pct_item = train_df.item_id.nunique() / item_count\n",
    "    test_pct_item = test_df.item_id.nunique() / item_count\n",
    "    train_pct_user = train_df.user_id.nunique() / user_count\n",
    "    test_pct_user = test_df.user_id.nunique() / user_count\n",
    "    print(f\"Train {train_df.shape} ({train_pct_total:.0%} total, {train_pct_item:.0%} items, {train_pct_user:.0%} users) \" +\n",
    "          f\"Test {test_df.shape} ({test_pct_total:.0%} total, {test_pct_item:.0%} items, {test_pct_user:.0%} users)\")\n",
    "    return train_df, test_df\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Splitting training/testing datasets for {dataset['Title']}\")\n",
    "    train_df, test_df = TrainTestSplit(dataset['df'])\n",
    "    dataset['train_df'] = train_df\n",
    "    dataset['test_df'] = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66dd2a-27c9-43f9-8781-5782bd421198",
   "metadata": {},
   "source": [
    "Now, for each training/testing data frame, encode the user ids to contiguous sets using LabelEncoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0045c5-ccad-422b-aaac-30701dafe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding user_id for MovieLens\n",
      "Distinct Training Users: 121,905\n",
      "Distinct Testing Users: 40,636\n",
      "Encoding user_id for Netflix\n",
      "Distinct Training Users: 355,362\n",
      "Distinct Testing Users: 118,454\n",
      "Encoding user_id for Yahoo! Music\n",
      "Distinct Training Users: 638,199\n",
      "Distinct Testing Users: 212,733\n",
      "Encoding user_id for BoardGameGeek\n",
      "Distinct Training Users: 249,059\n",
      "Distinct Testing Users: 83,020\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding user_id for {dataset['Title']}\")\n",
    "\n",
    "    train_df = dataset['train_df']\n",
    "    train_user_id_encoder = LabelEncoder()\n",
    "    train_user_id_encoder.fit(train_df.user_id)\n",
    "    dataset['train_user_id_encoder'] = train_user_id_encoder\n",
    "    n_train_users = train_user_id_encoder.classes_.size\n",
    "    dataset['n_train_users'] = n_train_users\n",
    "    train_df['user_idx'] = train_user_id_encoder.transform(train_df.user_id)\n",
    "    print(f\"Distinct Training Users: {n_train_users:,}\")\n",
    "\n",
    "    test_df = dataset['test_df']\n",
    "    test_user_id_encoder = LabelEncoder()\n",
    "    test_user_id_encoder.fit(test_df.user_id)\n",
    "    dataset['test_user_id_encoder'] = test_user_id_encoder\n",
    "    n_test_users = test_user_id_encoder.classes_.size\n",
    "    dataset['n_test_users'] = n_test_users\n",
    "    test_df['user_idx'] = test_user_id_encoder.transform(test_df.user_id)\n",
    "    print(f\"Distinct Testing Users: {n_test_users:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989f3c6-534c-49a7-bcae-6987dd391dc6",
   "metadata": {},
   "source": [
    "Now create a sparse matrix for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a529fa22-d031-42be-b091-0b5e5f959e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse training matrix for MovieLens\n",
      "Creating sparse training matrix for Netflix\n",
      "Creating sparse training matrix for Yahoo! Music\n",
      "Creating sparse training matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse training matrix for {dataset['Title']}\")\n",
    "    dataset['train_X'] = csr_matrix(\n",
    "        (dataset['train_df'].rating, (dataset['train_df'].user_idx, dataset['train_df'].item_idx)),\n",
    "        shape=(dataset['n_train_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31334078-351a-41a7-8bc0-239a57a4e340",
   "metadata": {},
   "source": [
    "Now we need to split the test dataset into a seen/unseen split (75/25), stratified by user id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d851b0ea-772b-4478-8e6a-0597740107d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating seen/unseen testing split MovieLens\n",
      "Seen (4637730, 7), Unseen (1545910, 7)\n",
      "Creating seen/unseen testing split Netflix\n",
      "Seen (9551190, 7), Unseen (3183730, 7)\n",
      "Creating seen/unseen testing split Yahoo! Music\n",
      "Seen (1223893, 7), Unseen (407965, 7)\n",
      "Creating seen/unseen testing split BoardGameGeek\n",
      "Seen (3557549, 7), Unseen (1185850, 7)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating seen/unseen testing split {dataset['Title']}\")\n",
    "    test_df = dataset['test_df']\n",
    "    seen_df, unseen_df = train_test_split(test_df, train_size=0.75, random_state=777, stratify=test_df.user_idx)\n",
    "    dataset['seen_df'] = seen_df\n",
    "    dataset['unseen_df'] = unseen_df\n",
    "    print(f\"Seen {seen_df.shape}, Unseen {unseen_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce5558-d2de-4c8b-8487-558a013f5429",
   "metadata": {},
   "source": [
    "Now we can create the sparse matrix for the testing seen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d277d07-8cec-4f50-86d5-121829913ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse testing/seen matrix for MovieLens\n",
      "Creating sparse testing/seen matrix for Netflix\n",
      "Creating sparse testing/seen matrix for Yahoo! Music\n",
      "Creating sparse testing/seen matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse testing/seen matrix for {dataset['Title']}\")\n",
    "    dataset['test_X'] = csr_matrix(\n",
    "        (dataset['seen_df'].rating, (dataset['seen_df'].user_idx, dataset['seen_df'].item_idx)),\n",
    "        shape=(dataset['n_test_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c251a-4cbb-43d3-ad21-653fe2880102",
   "metadata": {},
   "source": [
    "## kNN Analysis\n",
    "\n",
    "We will now use values for k ranging from 5 to 250 to find the optimal neihgborhood size for each dataset.  Since performing the predictions using knn takes some time (more than 1 minute for just k=5) we will save the resulting prediction datasets to feather files so we can recover in the event of an error.  NOTE: This happened (ran out of memory on the first pass when evaluating the k=250 datasets after starting the run overnight - phew!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c7cebf8-5d9a-4f40-a26c-623babd09036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute_pred_df(dataset, k):\n",
    "    cached_filename = f\"./pred_cache/{dataset['File']}.pred.k{k}.feather\"\n",
    "    try:\n",
    "        cached_df = pd.read_feather(cached_filename)\n",
    "        print('Using cached result (clear /pred_cache folder to recompute)')\n",
    "        return cached_df\n",
    "    except:\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n",
    "        print('Fitting kNN model on training data')\n",
    "        nn.fit(dataset['train_X'])\n",
    "        print('Finding neighbors for \"seen\" testing data')\n",
    "        neighbors = nn.kneighbors(dataset['test_X'], return_distance=False)\n",
    "        print('Melting neigbors')\n",
    "        melted_neighbors_df = pd.DataFrame(neighbors).melt(\n",
    "            value_name='neighbor_train_user_idx',\n",
    "            ignore_index=False\n",
    "        )[['neighbor_train_user_idx']].reset_index(names='test_user_idx')\n",
    "        print('Computing average ratings for the \"unseen\" testing data')\n",
    "        unseen_df = dataset['unseen_df']\n",
    "        merged_df = unseen_df\\\n",
    "            .merge(melted_neighbors_df, left_on='user_idx', right_on='test_user_idx')\\\n",
    "            .merge(train_df, left_on=['neighbor_train_user_idx', 'item_idx'], right_on=['user_idx', 'item_idx'], suffixes=['', '_train'])\n",
    "        averages_df = merged_df.groupby(['user_idx', 'item_idx'])[['rating_train']].mean()\n",
    "        all_pred_df = unseen_df.merge(averages_df, how='left', on=['user_idx', 'item_idx'])\\\n",
    "            [['item_id', 'user_id', 'rating', 'rating_bin', 'rating_train']]\n",
    "        print('Saving cached result to disk')\n",
    "        all_pred_df.to_feather(cached_filename)\n",
    "        return all_pred_df\n",
    "\n",
    "def knn_test(dataset, k):\n",
    "    print(f\"Testing neighborhood size k={k} for {dataset['Title']}\")\n",
    "    all_pred_df = load_or_compute_pred_df(dataset, k)\n",
    "    pred_df = all_pred_df[all_pred_df.rating_train.notnull()]\n",
    "    dataset[f\"all_pred_df_{k}\"] = all_pred_df\n",
    "    dataset[f\"pred_df_{k}\"] = pred_df\n",
    "    n_total = all_pred_df.shape[0]\n",
    "    n_pred = pred_df.shape[0]\n",
    "    n_null = n_total - n_pred\n",
    "    dataset[f\"n_total_{k}\"] = n_total\n",
    "    dataset[f\"n_pred_{k}\"] = n_pred\n",
    "    dataset[f\"n_null_{k}\"] = n_null\n",
    "    print(f\"Done. Predicted {n_pred} of {n_total} ({n_null} NaN)\")    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be04422-80b1-44f9-9fad-e7dc443ffe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neighborhood size k=5 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1155346 of 1545910 (390564 NaN)\n",
      "\n",
      "Testing neighborhood size k=5 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 69225 of 3183730 (3114505 NaN)\n",
      "\n",
      "Testing neighborhood size k=5 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 13331 of 407965 (394634 NaN)\n",
      "\n",
      "Testing neighborhood size k=5 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 20837 of 1185850 (1165013 NaN)\n",
      "\n",
      "Testing neighborhood size k=10 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1280716 of 1545910 (265194 NaN)\n",
      "\n",
      "Testing neighborhood size k=10 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 128844 of 3183730 (3054886 NaN)\n",
      "\n",
      "Testing neighborhood size k=10 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 25081 of 407965 (382884 NaN)\n",
      "\n",
      "Testing neighborhood size k=10 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 37882 of 1185850 (1147968 NaN)\n",
      "\n",
      "Testing neighborhood size k=25 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1393835 of 1545910 (152075 NaN)\n",
      "\n",
      "Testing neighborhood size k=25 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 268889 of 3183730 (2914841 NaN)\n",
      "\n",
      "Testing neighborhood size k=25 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 53169 of 407965 (354796 NaN)\n",
      "\n",
      "Testing neighborhood size k=25 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 75371 of 1185850 (1110479 NaN)\n",
      "\n",
      "Testing neighborhood size k=50 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1449293 of 1545910 (96617 NaN)\n",
      "\n",
      "Testing neighborhood size k=50 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 429718 of 3183730 (2754012 NaN)\n",
      "\n",
      "Testing neighborhood size k=50 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 85937 of 407965 (322028 NaN)\n",
      "\n",
      "Testing neighborhood size k=50 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 118879 of 1185850 (1066971 NaN)\n",
      "\n",
      "Testing neighborhood size k=100 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1487363 of 1545910 (58547 NaN)\n",
      "\n",
      "Testing neighborhood size k=100 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 640067 of 3183730 (2543663 NaN)\n",
      "\n",
      "Testing neighborhood size k=100 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 127383 of 407965 (280582 NaN)\n",
      "\n",
      "Testing neighborhood size k=100 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 175854 of 1185850 (1009996 NaN)\n",
      "\n",
      "Testing neighborhood size k=150 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1503219 of 1545910 (42691 NaN)\n",
      "\n",
      "Testing neighborhood size k=150 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 783607 of 3183730 (2400123 NaN)\n",
      "\n",
      "Testing neighborhood size k=150 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 154013 of 407965 (253952 NaN)\n",
      "\n",
      "Testing neighborhood size k=150 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 216130 of 1185850 (969720 NaN)\n",
      "\n",
      "Testing neighborhood size k=200 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1512061 of 1545910 (33849 NaN)\n",
      "\n",
      "Testing neighborhood size k=200 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 894469 of 3183730 (2289261 NaN)\n",
      "\n",
      "Testing neighborhood size k=200 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 173655 of 407965 (234310 NaN)\n",
      "\n",
      "Testing neighborhood size k=200 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 247530 of 1185850 (938320 NaN)\n",
      "\n",
      "Testing neighborhood size k=250 for MovieLens\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1517872 of 1545910 (28038 NaN)\n",
      "\n",
      "Testing neighborhood size k=250 for Netflix\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 468689 of 3183730 (2715041 NaN)\n",
      "\n",
      "Testing neighborhood size k=250 for Yahoo! Music\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 83230 of 407965 (324735 NaN)\n",
      "\n",
      "Testing neighborhood size k=250 for BoardGameGeek\n",
      "Using cached result (clear /pred_cache folder to recompute)\n",
      "Done. Predicted 1067333 of 1185850 (118517 NaN)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 25, 50, 100, 150, 200, 250]\n",
    "for k in ks:\n",
    "    for dataset in datasets:\n",
    "        knn_test(dataset, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7147090-109d-49d8-b214-136a584495bf",
   "metadata": {},
   "source": [
    "Next, we need to perform a linear regression to compute the R^2 value for each set of predictions.  We should also add the predicted size vs. testing size metric (to see how the unpredictable set changes with k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f77bfaf7-8d72-4a00-bc3f-d3fc645800ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScoreRatings(dataset, k):\n",
    "    df = dataset[f\"pred_df_{k}\"]\n",
    "    result = linregress(df.rating, df.rating_train)\n",
    "    rvalue = result.rvalue\n",
    "    coef_det = rvalue * rvalue\n",
    "    print(f\"k={k}, R^2 = {coef_det}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86ccab0e-0e6e-4966-843c-9e1384e796f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating R^2 for MovieLens\n",
      "k=5, R^2 = 0.1495154188384571\n",
      "k=10, R^2 = 0.1710952844882682\n",
      "k=25, R^2 = 0.19837371652315747\n",
      "k=50, R^2 = 0.21426213770362265\n",
      "k=100, R^2 = 0.22734575747851915\n",
      "k=150, R^2 = 0.23309274287158122\n",
      "k=200, R^2 = 0.23650984752459725\n",
      "k=250, R^2 = 0.23873341010063248\n",
      "\n",
      "Evaluating R^2 for Netflix\n",
      "k=5, R^2 = 0.0003092962723988484\n",
      "k=10, R^2 = 0.00017448519509314183\n",
      "k=25, R^2 = 0.00028483986626896417\n",
      "k=50, R^2 = 0.0004209217059557052\n",
      "k=100, R^2 = 0.00033947374880325396\n",
      "k=150, R^2 = 0.00034953068556711715\n",
      "k=200, R^2 = 0.0003000495175410974\n",
      "k=250, R^2 = 2.1981085558120076e-05\n",
      "\n",
      "Evaluating R^2 for Yahoo! Music\n",
      "k=5, R^2 = 6.736476279609527e-06\n",
      "k=10, R^2 = 0.00010823625326729013\n",
      "k=25, R^2 = 0.00014584170604428016\n",
      "k=50, R^2 = 0.00012428503385046921\n",
      "k=100, R^2 = 8.821046626184293e-05\n",
      "k=150, R^2 = 5.673372501940381e-05\n",
      "k=200, R^2 = 7.477032510212727e-05\n",
      "k=250, R^2 = 7.670232163740563e-05\n",
      "\n",
      "Evaluating R^2 for BoardGameGeek\n",
      "k=5, R^2 = 0.002363213472237026\n",
      "k=10, R^2 = 0.0029696486051719205\n",
      "k=25, R^2 = 0.002153078477266358\n",
      "k=50, R^2 = 0.0014465494148366844\n",
      "k=100, R^2 = 0.001060475496376932\n",
      "k=150, R^2 = 0.000804224565252748\n",
      "k=200, R^2 = 0.0006310130545643137\n",
      "k=250, R^2 = 0.2533813580232427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Evaluating R^2 for {dataset['Title']}\")\n",
    "    for k in ks:\n",
    "        ScoreRatings(dataset, k)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccab373-6080-4c57-abf0-b49c2b4669c1",
   "metadata": {},
   "source": [
    "These numbers do not look right.  MovieLens - maybe.  k=250 for BoardGameGeek - maybe.  But the rest are abysmal.\n",
    "\n",
    "Time to double check the calculations.  Let us take Netflix for k=100 and linearly run through the combinations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38f9d1a8-5dba-43d9-ae99-b5b103b6dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset: (51031355, 3)\n",
      "Filtering out users with only one rating: (51027153, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031350</th>\n",
       "      <td>9210</td>\n",
       "      <td>2420260</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031351</th>\n",
       "      <td>9210</td>\n",
       "      <td>761176</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031352</th>\n",
       "      <td>9210</td>\n",
       "      <td>459277</td>\n",
       "      <td>3</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031353</th>\n",
       "      <td>9210</td>\n",
       "      <td>2407365</td>\n",
       "      <td>4</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031354</th>\n",
       "      <td>9210</td>\n",
       "      <td>627867</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51027153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          item_id  user_id  rating  rating_count\n",
       "0               1  1488844       3          1127\n",
       "1               1   822109       5            75\n",
       "2               1   885013       4           181\n",
       "3               1    30878       4           676\n",
       "4               1   823519       3           324\n",
       "...           ...      ...     ...           ...\n",
       "51031350     9210  2420260       1           493\n",
       "51031351     9210   761176       3           134\n",
       "51031352     9210   459277       3           793\n",
       "51031353     9210  2407365       4           362\n",
       "51031354     9210   627867       3            89\n",
       "\n",
       "[51027153 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_feather('./datasets/netflix_prize.feather')\n",
    "print(f\"Initial dataset: {df.shape}\")\n",
    "counts_df = df.groupby('user_id')[['rating']].count()\n",
    "merged_df = df.merge(counts_df, on='user_id', suffixes=['', '_count'])\n",
    "filtered_df = merged_df[merged_df.rating_count > 1]\n",
    "multi_df = filtered_df.copy()\n",
    "print(f\"Filtering out users with only one rating: {multi_df.shape}\")\n",
    "multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d864d03d-d7f6-4c3a-a6e2-93b6e8ecdfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Item Count: 9,210\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1488844</td>\n",
       "      <td>3</td>\n",
       "      <td>1127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>822109</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>885013</td>\n",
       "      <td>4</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30878</td>\n",
       "      <td>4</td>\n",
       "      <td>676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>823519</td>\n",
       "      <td>3</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031350</th>\n",
       "      <td>9210</td>\n",
       "      <td>2420260</td>\n",
       "      <td>1</td>\n",
       "      <td>493</td>\n",
       "      <td>9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031351</th>\n",
       "      <td>9210</td>\n",
       "      <td>761176</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031352</th>\n",
       "      <td>9210</td>\n",
       "      <td>459277</td>\n",
       "      <td>3</td>\n",
       "      <td>793</td>\n",
       "      <td>9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031353</th>\n",
       "      <td>9210</td>\n",
       "      <td>2407365</td>\n",
       "      <td>4</td>\n",
       "      <td>362</td>\n",
       "      <td>9209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031354</th>\n",
       "      <td>9210</td>\n",
       "      <td>627867</td>\n",
       "      <td>3</td>\n",
       "      <td>89</td>\n",
       "      <td>9209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51027153 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          item_id  user_id  rating  rating_count  item_idx\n",
       "0               1  1488844       3          1127         0\n",
       "1               1   822109       5            75         0\n",
       "2               1   885013       4           181         0\n",
       "3               1    30878       4           676         0\n",
       "4               1   823519       3           324         0\n",
       "...           ...      ...     ...           ...       ...\n",
       "51031350     9210  2420260       1           493      9209\n",
       "51031351     9210   761176       3           134      9209\n",
       "51031352     9210   459277       3           793      9209\n",
       "51031353     9210  2407365       4           362      9209\n",
       "51031354     9210   627867       3            89      9209\n",
       "\n",
       "[51027153 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_encoder = LabelEncoder()\n",
    "item_id_encoder.fit(multi_df.item_id)\n",
    "n_items = item_id_encoder.classes_.size\n",
    "print(f\"Distinct Item Count: {n_items:,}\")\n",
    "multi_df['item_idx'] = item_id_encoder.transform(multi_df.item_id)\n",
    "multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "862bfa64-8ff5-465c-a353-d25b170813e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (38292233, 5) (75% total, 100% items, 75% users) Test (12734920, 5) (25% total, 100% items, 25% users)\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=777)\n",
    "train_index, test_index = next(gss.split(X=multi_df, y=multi_df.rating, groups=multi_df.user_id))\n",
    "train_df = multi_df.iloc[train_index].copy()\n",
    "test_df = multi_df.iloc[test_index].copy()\n",
    "total_count = train_df.shape[0] + test_df.shape[0];\n",
    "item_count = multi_df.item_idx.nunique()\n",
    "user_count = multi_df.user_id.nunique()\n",
    "train_pct_total = train_df.shape[0] / total_count\n",
    "test_pct_total = test_df.shape[0] / total_count\n",
    "train_pct_item = train_df.item_idx.nunique() / item_count\n",
    "test_pct_item = test_df.item_idx.nunique() / item_count\n",
    "train_pct_user = train_df.user_id.nunique() / user_count\n",
    "test_pct_user = test_df.user_id.nunique() / user_count\n",
    "print(f\"Train {train_df.shape} ({train_pct_total:.0%} total, {train_pct_item:.0%} items, {train_pct_user:.0%} users) \" +\n",
    "      f\"Test {test_df.shape} ({test_pct_total:.0%} total, {test_pct_item:.0%} items, {test_pct_user:.0%} users)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a5ba790-c1d4-4c8b-ac1d-ef04bb913ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Training Users: 355,362\n",
      "Distinct Testing Users: 118,454\n"
     ]
    }
   ],
   "source": [
    "train_user_id_encoder = LabelEncoder()\n",
    "train_user_id_encoder.fit(train_df.user_id)\n",
    "n_train_users = train_user_id_encoder.classes_.size\n",
    "train_df['user_idx'] = train_user_id_encoder.transform(train_df.user_id)\n",
    "print(f\"Distinct Training Users: {n_train_users:,}\")\n",
    "\n",
    "test_user_id_encoder = LabelEncoder()\n",
    "test_user_id_encoder.fit(test_df.user_id)\n",
    "n_test_users = test_user_id_encoder.classes_.size\n",
    "test_df['user_idx'] = test_user_id_encoder.transform(test_df.user_id)\n",
    "print(f\"Distinct Testing Users: {n_test_users:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "561d4f5b-821e-4a66-9598-c97c32135dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = csr_matrix(\n",
    "    (train_df.rating, (train_df.user_idx, train_df.item_idx)),\n",
    "    shape=(n_train_users, n_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd57bc1b-0b65-49cc-bbb1-8fd9312b5a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen (9551190, 6), Unseen (3183730, 6)\n"
     ]
    }
   ],
   "source": [
    "seen_df, unseen_df = train_test_split(test_df, train_size=0.75, random_state=777, stratify=test_df.user_idx)\n",
    "print(f\"Seen {seen_df.shape}, Unseen {unseen_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a630948-cb77-47af-ac7b-3d6033de8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = csr_matrix(\n",
    "    (seen_df.rating, (seen_df.user_idx, seen_df.item_idx)),\n",
    "    shape=(n_test_users, n_items)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "369a84c4-4ac6-4584-9996-ef014a154874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neighborhood size k=100 for MovieLens\n",
      "Fitting kNN model on training data\n",
      "Finding neighbors for \"seen\" testing data\n",
      "Melting neigbors\n",
      "Computing average ratings for the \"unseen\" testing data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['rating_bin'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m unseen_df\\\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mmerge(melted_neighbors_df, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_user_idx\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mmerge(train_df, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbor_train_user_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], right_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_idx\u001b[39m\u001b[38;5;124m'\u001b[39m], suffixes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m averages_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_idx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating_train\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 18\u001b[0m all_pred_df \u001b[38;5;241m=\u001b[39m \u001b[43munseen_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverages_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating_bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m all_pred_df[all_pred_df\u001b[38;5;241m.\u001b[39mrating_train\u001b[38;5;241m.\u001b[39mnotnull()]\n\u001b[0;32m     21\u001b[0m n_total \u001b[38;5;241m=\u001b[39m all_pred_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['rating_bin'] not in index\""
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "print(f\"Testing neighborhood size k={k} for Netflix\")\n",
    "nn = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n",
    "print('Fitting kNN model on training data')\n",
    "nn.fit(train_X)\n",
    "print('Finding neighbors for \"seen\" testing data')\n",
    "neighbors = nn.kneighbors(test_X, return_distance=False)\n",
    "print('Melting neigbors')\n",
    "melted_neighbors_df = pd.DataFrame(neighbors).melt(\n",
    "    value_name='neighbor_train_user_idx',\n",
    "    ignore_index=False\n",
    ")[['neighbor_train_user_idx']].reset_index(names='test_user_idx')\n",
    "print('Computing average ratings for the \"unseen\" testing data')\n",
    "merged_df = unseen_df\\\n",
    "    .merge(melted_neighbors_df, left_on='user_idx', right_on='test_user_idx')\\\n",
    "    .merge(train_df, left_on=['neighbor_train_user_idx', 'item_idx'], right_on=['user_idx', 'item_idx'], suffixes=['', '_train'])\n",
    "averages_df = merged_df.groupby(['user_idx', 'item_idx'])[['rating_train']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491401d2-1dc2-404b-a603-7bc80c5a859e",
   "metadata": {},
   "source": [
    "The above command took around 15m to find neighbors, and around 18m in total to find predictions (output says MovieLens, but it is totally Netflix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ee186b9-0bd4-4d6b-bb8f-5ea0471a58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Predicted 2997931 of 3183730 (185799 NaN)\n"
     ]
    }
   ],
   "source": [
    "all_pred_df = unseen_df.merge(averages_df, how='left', on=['user_idx', 'item_idx'])\\\n",
    "    [['item_id', 'user_id', 'rating', 'rating_train']]\n",
    "pred_df = all_pred_df[all_pred_df.rating_train.notnull()]\n",
    "n_total = all_pred_df.shape[0]\n",
    "n_pred = pred_df.shape[0]\n",
    "n_null = n_total - n_pred\n",
    "print(f\"Done. Predicted {n_pred} of {n_total} ({n_null} NaN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59e98701-ff6f-4f29-ac78-ef0644a3bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=100, R^2 = 0.19736097082472437\n"
     ]
    }
   ],
   "source": [
    "pred_df\n",
    "result = linregress(pred_df.rating, pred_df.rating_train)\n",
    "rvalue = result.rvalue\n",
    "coef_det = rvalue * rvalue\n",
    "print(f\"k={k}, R^2 = {coef_det}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb097b2-4ce9-48cc-8c79-67c254683b04",
   "metadata": {},
   "source": [
    "WTF.  That looks fine.  There is obviously something wrong with the code or logic the way it was written because copying the single-pass for Netflix with k=100 seems to work fine (0.19 seems reasonable, vs. 0.00033947374880325396 which is atrocious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9637603-f916-4eaf-b343-fdf753f9d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadOrComputePrediction(name, k, train_X, test_X, unseen_df, train_df):\n",
    "    cached_filename = f\"./pred_cache/{name}.pred.v2.k{k}.feather\"\n",
    "    try:\n",
    "        cached_df = pd.read_feather(cached_filename)\n",
    "        print('Using cached result (clear /pred_cache folder to recompute)')\n",
    "        return cached_df\n",
    "    except:\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n",
    "        print('Fitting kNN model on training data')\n",
    "        nn.fit(train_X)\n",
    "        print('Finding neighbors for \"seen\" testing data')\n",
    "        neighbors = nn.kneighbors(test_X, return_distance=False)\n",
    "        print('Melting neigbors')\n",
    "        melted_neighbors_df = pd.DataFrame(neighbors).melt(\n",
    "            value_name='neighbor_train_user_idx',\n",
    "            ignore_index=False\n",
    "        )[['neighbor_train_user_idx']].reset_index(names='test_user_idx')\n",
    "        print('Computing average ratings for the \"unseen\" testing data')\n",
    "        merged_df = unseen_df\\\n",
    "            .merge(melted_neighbors_df, left_on='user_idx', right_on='test_user_idx')\\\n",
    "            .merge(train_df, left_on=['neighbor_train_user_idx', 'item_idx'], right_on=['user_idx', 'item_idx'], suffixes=['', '_train'])\n",
    "        averages_df = merged_df.groupby(['user_idx', 'item_idx'])[['rating_train']].mean()        \n",
    "        all_pred_df = unseen_df.merge(averages_df, how='left', on=['user_idx', 'item_idx'])\\\n",
    "            [['item_id', 'user_id', 'rating', 'rating_train']]\n",
    "        print('Saving cached result to disk')\n",
    "        all_pred_df.to_feather(cached_filename)\n",
    "        return all_pred_df    \n",
    "\n",
    "def AnalyzeDataset(title, name):\n",
    "    print(f\"Analyzing dataset for {title}\")\n",
    "    df = pd.read_feather(f'./datasets/{name}.feather')\n",
    "    print(f\"Initial dataset: {df.shape}\")\n",
    "    counts_df = df.groupby('user_id')[['rating']].count()\n",
    "    merged_df = df.merge(counts_df, on='user_id', suffixes=['', '_count'])\n",
    "    filtered_df = merged_df[merged_df.rating_count > 1]\n",
    "    multi_df = filtered_df.copy()\n",
    "    print(f\"Filtering out users with only one rating: {multi_df.shape}\")\n",
    "\n",
    "    item_id_encoder = LabelEncoder()\n",
    "    item_id_encoder.fit(multi_df.item_id)\n",
    "    n_items = item_id_encoder.classes_.size\n",
    "    print(f\"Distinct Item Count: {n_items:,}\")\n",
    "    multi_df['item_idx'] = item_id_encoder.transform(multi_df.item_id)\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=777)\n",
    "    train_index, test_index = next(gss.split(X=multi_df, y=multi_df.rating, groups=multi_df.user_id))\n",
    "    train_df = multi_df.iloc[train_index].copy()\n",
    "    test_df = multi_df.iloc[test_index].copy()\n",
    "    print(f\"Train {train_df.shape}, Test {test_df.shape}\")\n",
    "\n",
    "    train_user_id_encoder = LabelEncoder()\n",
    "    train_user_id_encoder.fit(train_df.user_id)\n",
    "    n_train_users = train_user_id_encoder.classes_.size\n",
    "    train_df['user_idx'] = train_user_id_encoder.transform(train_df.user_id)\n",
    "    print(f\"Distinct Training Users: {n_train_users:,}\")\n",
    "    \n",
    "    test_user_id_encoder = LabelEncoder()\n",
    "    test_user_id_encoder.fit(test_df.user_id)\n",
    "    n_test_users = test_user_id_encoder.classes_.size\n",
    "    test_df['user_idx'] = test_user_id_encoder.transform(test_df.user_id)\n",
    "    print(f\"Distinct Testing Users: {n_test_users:,}\")\n",
    "\n",
    "    train_X = csr_matrix(\n",
    "        (train_df.rating, (train_df.user_idx, train_df.item_idx)),\n",
    "        shape=(n_train_users, n_items)\n",
    "    )\n",
    "\n",
    "    seen_df, unseen_df = train_test_split(test_df, train_size=0.75, random_state=777, stratify=test_df.user_idx)\n",
    "    print(f\"Seen {seen_df.shape}, Unseen {unseen_df.shape}\")\n",
    "\n",
    "    test_X = csr_matrix(\n",
    "        (seen_df.rating, (seen_df.user_idx, seen_df.item_idx)),\n",
    "        shape=(n_test_users, n_items)\n",
    "    )\n",
    "\n",
    "    print('')\n",
    "\n",
    "    for k in [5, 10, 25, 50, 100, 150, 200, 250]:\n",
    "        print(f\"Testing neighborhood size k={k}\")\n",
    "        all_pred_df = LoadOrComputePrediction(name, k, train_X, test_X, unseen_df, train_df)\n",
    "        pred_df = all_pred_df[all_pred_df.rating_train.notnull()]\n",
    "        n_total = all_pred_df.shape[0]\n",
    "        n_pred = pred_df.shape[0]\n",
    "        n_null = n_total - n_pred\n",
    "        print(f\"Done. Predicted {n_pred} of {n_total} ({n_null} NaN)\")\n",
    "\n",
    "        result = linregress(pred_df.rating, pred_df.rating_train)\n",
    "        rvalue = result.rvalue\n",
    "        coef_det = rvalue * rvalue\n",
    "        print(f\"k={k}, R^2 = {coef_det}\")\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccdcaa-11ea-49b7-81f1-1aed1a256a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset for Netflix\n",
      "Initial dataset: (51031355, 3)\n",
      "Filtering out users with only one rating: (51027153, 4)\n",
      "Distinct Item Count: 9,210\n",
      "Train (38292233, 5), Test (12734920, 5)\n",
      "Distinct Training Users: 355,362\n",
      "Distinct Testing Users: 118,454\n",
      "Seen (9551190, 6), Unseen (3183730, 6)\n",
      "Fitting kNN model on training data\n",
      "Finding neighbors for \"seen\" testing data\n"
     ]
    }
   ],
   "source": [
    "AnalyzeDataset('Netflix', 'netflix_prize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85050d-d19b-4ebd-9233-b5cd13790e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
