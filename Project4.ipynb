{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d20c0a-a47e-4e52-95e5-02289c0e21c2",
   "metadata": {},
   "source": [
    "# CSC6711 Project 4 - Collaborative Filtering with kNN\n",
    "* **Author:** Jacob Buysse\n",
    "\n",
    "This notebook is an analysis of the predictions based on user clustering using kNN.\n",
    "The files are located in the datasets subdirectory:\n",
    "\n",
    "* MovieLens - `movielens_25m.feather` (Movies)\n",
    "* Netflix Prize - `netflix_prize.feather` (Movies and TV Shows)\n",
    "* Yahoo! Music R2 - `yahoo_r2_songs.subsampled.feather` (Songs)\n",
    "* BoardGameGeek - `boardgamegeek.feather` (Board Games)\n",
    "\n",
    "We will be using the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b34c046a-057c-445d-968f-9c704927088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03909103-b288-46cb-a2ee-db0b54ac1ca4",
   "metadata": {},
   "source": [
    "Let us configure matplotlib for readable labels, high resolution, and automatic layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc1e42e-1c37-45c9-9105-e4e88a97e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('axes', labelsize=16)\n",
    "matplotlib.rc('figure', dpi=150, autolayout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e39bc-aba9-4bf1-8847-41e05f0c2e9e",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Let us load the 4 datasets.  We will proceed to clean, filter, preprocess, and split the datasets before continuing on to the kNN portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ed13165-04a0-426e-b06e-a74e5ce0df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens...\n",
      "Shape (24890583, 4)\n",
      "Loading Netflix...\n",
      "Shape (51031355, 4)\n",
      "Loading Yahoo! Music...\n",
      "Shape (6937275, 4)\n",
      "Loading BoardGameGeek...\n",
      "Encoding user_id: string -> int64\n",
      "Shape (18942215, 4)\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    { 'Title': 'MovieLens', 'File': 'movielens_25m' },\n",
    "    { 'Title': 'Netflix', 'File': 'netflix_prize' },\n",
    "    { 'Title': 'Yahoo! Music', 'File': 'yahoo_r2_songs.subsampled' },\n",
    "    { 'Title': 'BoardGameGeek', 'File': 'boardgamegeek' }\n",
    "]\n",
    "for dataset in datasets:\n",
    "    # Load the file\n",
    "    print(f\"Loading {dataset['Title']}...\")\n",
    "    df = pd.read_feather(f\"./datasets/{dataset['File']}.feather\")\n",
    "\n",
    "    # Add a rating_bin (floor of the rating) for graphing bins later\n",
    "    df['rating_bin'] = np.floor(df.rating)\n",
    "\n",
    "    # Use the label encoder to convert user_id into a numeric when it is a string (object)\n",
    "    # NOTE: This is needed for the BoardGameGeek dataset\n",
    "    if (df.user_id.dtype == object):\n",
    "        print('Encoding user_id: string -> int64')\n",
    "        user_id_encoder = LabelEncoder()\n",
    "        user_id_encoder.fit(df.user_id)\n",
    "        dataset['user_id_encoder'] = user_id_encoder\n",
    "        df['user_id'] = user_id_encoder.transform(df.user_id)\n",
    "\n",
    "    # Store the df in the dataset dictionary\n",
    "    dataset['df'] = df\n",
    "    print(f\"Shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e76b2-ff5f-4d9f-8c99-9dd2dca5ea72",
   "metadata": {},
   "source": [
    "Because of how we are doing our training/testing split and then seen/unseen split for testing, we need to exclude all users that only have a single rating.  This is because we cannot split those users in the testing dataset into both seen (for matching neighbors) and unseen (for prediction analysis).  Let us filter those users out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f10461f9-da4e-4b0e-93cd-24a9f2813905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out users with only one rating for MovieLens\n",
      "New Shape: (24890583, 5)\n",
      "Filtering out users with only one rating for Netflix\n",
      "New Shape: (51027153, 5)\n",
      "Filtering out users with only one rating for Yahoo! Music\n",
      "New Shape: (6532945, 5)\n",
      "Filtering out users with only one rating for BoardGameGeek\n",
      "New Shape: (18862919, 5)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Filtering out users with only one rating for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    counts_df = df.groupby('user_id')[['rating']].count()\n",
    "    merged_df = df.merge(counts_df, on='user_id', suffixes=['', '_count'])\n",
    "    filtered_df = merged_df[merged_df.rating_count > 1]\n",
    "    dataset['df'] = filtered_df.copy()\n",
    "    print(f\"New Shape: {filtered_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd883aeb-28a9-4bdd-a7b3-7a6693f25345",
   "metadata": {},
   "source": [
    "Let us encode the `item_id` column into a contiguous 0...n-1 range `item_idx` using a LabelEncoder.\n",
    "This will be used for the columns of the sparse matrices.\n",
    "Note that this encoding will be shared between the training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e6aa8b7-fc71-46fd-887e-17d715318453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id for MovieLens\n",
      "Distinct Item Count: 24,330\n",
      "Encoding item_id for Netflix\n",
      "Distinct Item Count: 9,210\n",
      "Encoding item_id for Yahoo! Music\n",
      "Distinct Item Count: 1,368\n",
      "Encoding item_id for BoardGameGeek\n",
      "Distinct Item Count: 21,925\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding item_id for {dataset['Title']}\")\n",
    "    df = dataset['df']\n",
    "    item_id_encoder = LabelEncoder()\n",
    "    item_id_encoder.fit(df.item_id)\n",
    "    dataset['item_id_encoder'] = item_id_encoder\n",
    "    n_items = item_id_encoder.classes_.size\n",
    "    dataset['n_items'] = n_items\n",
    "    df['item_idx'] = item_id_encoder.transform(df.item_id)\n",
    "    print(f\"Distinct Item Count: {n_items:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82f9ee-8cfe-41ee-8442-86968fa4818c",
   "metadata": {},
   "source": [
    "Let us do a 75/25 split for the training/testing datasets, split across the user ids as groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8988d88-69d5-4618-92d2-f5222e985986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting training/testing datasets for MovieLens\n",
      "Train (18706943, 6) (75% total, 100% items, 75% users) Test (6183640, 6) (25% total, 99% items, 25% users)\n",
      "Splitting training/testing datasets for Netflix\n",
      "Train (38292233, 6) (75% total, 100% items, 75% users) Test (12734920, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for Yahoo! Music\n",
      "Train (4901087, 6) (75% total, 100% items, 75% users) Test (1631858, 6) (25% total, 100% items, 25% users)\n",
      "Splitting training/testing datasets for BoardGameGeek\n",
      "Train (14119520, 6) (75% total, 100% items, 75% users) Test (4743399, 6) (25% total, 100% items, 25% users)\n"
     ]
    }
   ],
   "source": [
    "def TrainTestSplit(df):\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.75, random_state=777)\n",
    "    train_index, test_index = next(gss.split(X=df, y=df.rating, groups=df.user_id))\n",
    "    train_df = df.iloc[train_index].copy()\n",
    "    test_df = df.iloc[test_index].copy()\n",
    "    total_count = train_df.shape[0] + test_df.shape[0];\n",
    "    item_count = df.item_id.nunique()\n",
    "    user_count = df.user_id.nunique()\n",
    "    train_pct_total = train_df.shape[0] / total_count\n",
    "    test_pct_total = test_df.shape[0] / total_count\n",
    "    train_pct_item = train_df.item_id.nunique() / item_count\n",
    "    test_pct_item = test_df.item_id.nunique() / item_count\n",
    "    train_pct_user = train_df.user_id.nunique() / user_count\n",
    "    test_pct_user = test_df.user_id.nunique() / user_count\n",
    "    print(f\"Train {train_df.shape} ({train_pct_total:.0%} total, {train_pct_item:.0%} items, {train_pct_user:.0%} users) \" +\n",
    "          f\"Test {test_df.shape} ({test_pct_total:.0%} total, {test_pct_item:.0%} items, {test_pct_user:.0%} users)\")\n",
    "    return train_df, test_df\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Splitting training/testing datasets for {dataset['Title']}\")\n",
    "    train_df, test_df = TrainTestSplit(dataset['df'])\n",
    "    dataset['train_df'] = train_df\n",
    "    dataset['test_df'] = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66dd2a-27c9-43f9-8781-5782bd421198",
   "metadata": {},
   "source": [
    "Now, for each training/testing data frame, encode the user ids to contiguous sets using LabelEncoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c0045c5-ccad-422b-aaac-30701dafe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding user_id for MovieLens\n",
      "Distinct Training Users: 121,905\n",
      "Distinct Testing Users: 40,636\n",
      "Encoding user_id for Netflix\n",
      "Distinct Training Users: 355,362\n",
      "Distinct Testing Users: 118,454\n",
      "Encoding user_id for Yahoo! Music\n",
      "Distinct Training Users: 638,199\n",
      "Distinct Testing Users: 212,733\n",
      "Encoding user_id for BoardGameGeek\n",
      "Distinct Training Users: 249,059\n",
      "Distinct Testing Users: 83,020\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Encoding user_id for {dataset['Title']}\")\n",
    "\n",
    "    train_df = dataset['train_df']\n",
    "    train_user_id_encoder = LabelEncoder()\n",
    "    train_user_id_encoder.fit(train_df.user_id)\n",
    "    dataset['train_user_id_encoder'] = train_user_id_encoder\n",
    "    n_train_users = train_user_id_encoder.classes_.size\n",
    "    dataset['n_train_users'] = n_train_users\n",
    "    train_df['user_idx'] = train_user_id_encoder.transform(train_df.user_id)\n",
    "    print(f\"Distinct Training Users: {n_train_users:,}\")\n",
    "\n",
    "    test_df = dataset['test_df']\n",
    "    test_user_id_encoder = LabelEncoder()\n",
    "    test_user_id_encoder.fit(test_df.user_id)\n",
    "    dataset['test_user_id_encoder'] = test_user_id_encoder\n",
    "    n_test_users = test_user_id_encoder.classes_.size\n",
    "    dataset['n_test_users'] = n_test_users\n",
    "    test_df['user_idx'] = test_user_id_encoder.transform(test_df.user_id)\n",
    "    print(f\"Distinct Testing Users: {n_test_users:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989f3c6-534c-49a7-bcae-6987dd391dc6",
   "metadata": {},
   "source": [
    "Now create a sparse matrix for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a529fa22-d031-42be-b091-0b5e5f959e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse training matrix for MovieLens\n",
      "Creating sparse training matrix for Netflix\n",
      "Creating sparse training matrix for Yahoo! Music\n",
      "Creating sparse training matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse training matrix for {dataset['Title']}\")\n",
    "    dataset['train_X'] = csr_matrix(\n",
    "        (dataset['train_df'].rating, (dataset['train_df'].user_idx, dataset['train_df'].item_idx)),\n",
    "        shape=(dataset['n_train_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31334078-351a-41a7-8bc0-239a57a4e340",
   "metadata": {},
   "source": [
    "Now we need to split the test dataset into a seen/unseen split (75/25), stratified by user id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d851b0ea-772b-4478-8e6a-0597740107d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating seen/unseen testing split MovieLens\n",
      "Seen (4637730, 7), Unseen (1545910, 7)\n",
      "Creating seen/unseen testing split Netflix\n",
      "Seen (9551190, 7), Unseen (3183730, 7)\n",
      "Creating seen/unseen testing split Yahoo! Music\n",
      "Seen (1223893, 7), Unseen (407965, 7)\n",
      "Creating seen/unseen testing split BoardGameGeek\n",
      "Seen (3557549, 7), Unseen (1185850, 7)\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating seen/unseen testing split {dataset['Title']}\")\n",
    "    test_df = dataset['test_df']\n",
    "    seen_df, unseen_df = train_test_split(test_df, train_size=0.75, random_state=777, stratify=test_df.user_idx)\n",
    "    dataset['seen_df'] = seen_df\n",
    "    dataset['unseen_df'] = unseen_df\n",
    "    print(f\"Seen {seen_df.shape}, Unseen {unseen_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bce5558-d2de-4c8b-8487-558a013f5429",
   "metadata": {},
   "source": [
    "Now we can create the sparse matrix for the testing seen dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d277d07-8cec-4f50-86d5-121829913ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse testing/seen matrix for MovieLens\n",
      "Creating sparse testing/seen matrix for Netflix\n",
      "Creating sparse testing/seen matrix for Yahoo! Music\n",
      "Creating sparse testing/seen matrix for BoardGameGeek\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Creating sparse testing/seen matrix for {dataset['Title']}\")\n",
    "    dataset['test_X'] = csr_matrix(\n",
    "        (dataset['seen_df'].rating, (dataset['seen_df'].user_idx, dataset['seen_df'].item_idx)),\n",
    "        shape=(dataset['n_test_users'], dataset['n_items'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c251a-4cbb-43d3-ad21-653fe2880102",
   "metadata": {},
   "source": [
    "## kNN Analysis\n",
    "\n",
    "We will now use values for k ranging from 5 to 250 to find the optimal neihgborhood size for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8c7cebf8-5d9a-4f40-a26c-623babd09036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_compute_pred_df(dataset, k):\n",
    "    cached_filename = f\"./pred_cache/{dataset['File']}.pred.k{k}.feather\"\n",
    "    try:\n",
    "        cached_df = pd.read_feather(cached_filename)\n",
    "        print('Using cached result (clear /pred_cache folder to recompute)')\n",
    "        return cached_df\n",
    "    except:\n",
    "        nn = NearestNeighbors(n_neighbors=k, metric='cosine', n_jobs=-1)\n",
    "        print('Fitting kNN model on training data')\n",
    "        nn.fit(dataset['train_X'])\n",
    "        print('Finding neighbors for \"seen\" testing data')\n",
    "        neighbors = nn.kneighbors(dataset['test_X'], return_distance=False)\n",
    "        print('Melting neigbors')\n",
    "        melted_neighbors_df = pd.DataFrame(neighbors).melt(\n",
    "            value_name='neighbor_train_user_idx',\n",
    "            ignore_index=False\n",
    "        )[['neighbor_train_user_idx']].reset_index(names='test_user_idx')\n",
    "        print('Computing average ratings for the \"unseen\" testing data')\n",
    "        unseen_df = dataset['unseen_df']\n",
    "        merged_df = unseen_df\\\n",
    "            .merge(melted_neighbors_df, left_on='user_idx', right_on='test_user_idx')\\\n",
    "            .merge(train_df, left_on=['neighbor_train_user_idx', 'item_idx'], right_on=['user_idx', 'item_idx'], suffixes=['', '_train'])\n",
    "        averages_df = merged_df.groupby(['user_idx', 'item_idx'])[['rating_train']].mean()\n",
    "        all_pred_df = unseen_df.merge(averages_df, how='left', on=['user_idx', 'item_idx'])\\\n",
    "            [['item_id', 'user_id', 'rating', 'rating_bin', 'rating_train']]\n",
    "        print('Saving cached result to disk')\n",
    "        all_pred_df.to_feather(cached_filename)\n",
    "        return all_pred_df\n",
    "\n",
    "def knn_test(dataset, k):\n",
    "    print(f\"Testing neighborhood size k={k} for {dataset['Title']}\")\n",
    "    all_pred_df = load_or_compute_pred_df(dataset, k)\n",
    "    pred_df = all_pred_df[all_pred_df.rating_train.notnull()]\n",
    "    dataset['all_pred_df'] = all_pred_df\n",
    "    dataset['pred_df'] = pred_df\n",
    "    n_total = all_pred_df.shape[0]\n",
    "    n_pred = pred_df.shape[0]\n",
    "    n_null = n_total - n_pred\n",
    "    dataset['n_total'] = n_total\n",
    "    dataset['n_pred'] = n_pred\n",
    "    dataset['n_null'] = n_null\n",
    "    print(f\"Done. Predicted {n_pred} of {n_total} ({n_null} NaN)\")    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2be04422-80b1-44f9-9fad-e7dc443ffe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neighborhood size k=5 for MovieLens\n",
      "Using cached result (clear /datasets *pred* files to recompute)\n",
      "Done. Predicted 1155346 of 1545910 (390564 NaN)\n",
      "\n",
      "Testing neighborhood size k=5 for Netflix\n",
      "Fitting kNN model on training data\n",
      "Finding neighbors for \"seen\" testing data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 4\u001b[0m, in \u001b[0;36mload_or_compute_pred_df\u001b[1;34m(dataset, k)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     cached_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing cached result (clear /datasets *pred* files to recompute)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\pandas\\io\\feather_format.py:120\u001b[0m, in \u001b[0;36mread_feather\u001b[1;34m(path, columns, use_threads, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    118\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pyarrow_string_dtype():\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/netflix_prize.pred.k5.feather'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ks:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mknn_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[130], line 32\u001b[0m, in \u001b[0;36mknn_test\u001b[1;34m(dataset, k)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mknn_test\u001b[39m(dataset, k):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting neighborhood size k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     all_pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_compute_pred_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     pred_df \u001b[38;5;241m=\u001b[39m all_pred_df[all_pred_df\u001b[38;5;241m.\u001b[39mrating_train\u001b[38;5;241m.\u001b[39mnotnull()]\n\u001b[0;32m     34\u001b[0m     dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_pred_df\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m all_pred_df\n",
      "Cell \u001b[1;32mIn[130], line 12\u001b[0m, in \u001b[0;36mload_or_compute_pred_df\u001b[1;34m(dataset, k)\u001b[0m\n\u001b[0;32m     10\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_X\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinding neighbors for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseen\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m testing data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_X\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMelting neigbors\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m melted_neighbors_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(neighbors)\u001b[38;5;241m.\u001b[39mmelt(\n\u001b[0;32m     15\u001b[0m     value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbor_train_user_idx\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneighbor_train_user_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_user_idx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:886\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\n\u001b[1;32m--> 886\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2172\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2171\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[1;32m-> 2172\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   2174\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2175\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   2176\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   2178\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2375\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2372\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   2373\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1898\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1896\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n\u001b[0;32m   1897\u001b[0m ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1898\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_n_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[0;32m   1905\u001b[0m     \u001b[38;5;66;03m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[0;32m   1906\u001b[0m     np\u001b[38;5;241m.\u001b[39mfill_diagonal(ret, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\git\\csc6711\\project4\\msoe\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ks = [5, 10, 25, 50, 100, 150, 200, 250]\n",
    "for k in ks:\n",
    "    for dataset in datasets:\n",
    "        knn_test(dataset, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cacb3-9da3-4c69-aa95-3b6c455b46dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
